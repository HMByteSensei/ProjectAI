# -*- coding: utf-8 -*-
"""Training22visestrukiFAJLOVI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hvVVavIFpPWGlqYM5OLwYg_RmIrw97oO
"""

# Istekla veza pa pise greska dole pri pokretanju iako je nije bilo
# slucajno opet pokrenuo kod i ne prepoznaje sta je df u fazi 5, ali kada se ide redom radi sve

# ==============================================================================
# FAZA 1: PRIPREMA OKRUŽENJA
# ==============================================================================
# Instaliramo ključne biblioteke.
# 'transformers' i 'datasets' su iz Hugging Face ekosistema.
# 'sentencepiece' je potreban za mT5 tokenizer.
# 'evaluate' i 'rouge_score' su za evaluaciju.
!pip install transformers[torch] datasets sentencepiece evaluate rouge_score -q

import pandas as pd
import os
import glob # Za lako pronalaženje svih CSV datoteka
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
import torch
import evaluate # Biblioteka za evaluaciju od Hugging Face

from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)

# Provjerimo da li je GPU dostupan, što je ključno za brzinu treniranja
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Koristi se uređaj: {device}")

# ==============================================================================
# FAZA 2: UČITAVANJE I SPAJANJE VIŠE JSON FAJLOVA
# ==============================================================================
import pandas as pd
from google.colab import files
import glob
import os

# Kreiramo privremeni folder za upload
upload_dir = 'json_uploads'
if not os.path.exists(upload_dir):
    os.makedirs(upload_dir)

# Prelazimo u taj direktorij da bi uploadovani fajlovi završili tu
os.chdir(upload_dir)

# Pitamo korisnika da uploaduje sve željene JSON fajlove
print(f"Molimo, uploadujte SVE JSON fajlove koje želite koristiti za trening.")
print("Možete ih odabrati sve odjednom.")
uploaded_files = files.upload()

# Vraćamo se u glavni direktorij
os.chdir('..')

# Provjeravamo da li je išta uploadovano
if not uploaded_files:
    print("\nNijedan fajl nije uploadovan. Prekidam izvršavanje.")
else:
    print(f"\nUspješno ste uploadovali {len(uploaded_files)} fajlova.")

    # Lista za spremanje svih DataFrame-ova
    df_list = []

    # Pronalazimo sve JSON fajlove u upload folderu
    json_path = os.path.join(upload_dir, "*.json")
    all_json_files = glob.glob(json_path)

    # Učitavamo svaki JSON fajl i dodajemo ga u listu
    for file_path in all_json_files:
        try:
            print(f"Učitavam fajl: {os.path.basename(file_path)}...")
            temp_df = pd.read_json(file_path, encoding='utf-8')
            df_list.append(temp_df)
            print(f" -> Učitano {len(temp_df)} redova.")
        except Exception as e:
            print(f"GREŠKA pri učitavanju fajla {file_path}: {e}")

    # Spajamo sve DataFrame-ove u jedan veliki
    if df_list:
        df = pd.concat(df_list, ignore_index=True)
        print("\nSvi podaci su uspješno spojeni.")
        print(f"UKUPAN BROJ ČLANAKA ZA TRENING: {len(df)}")
        print("\nKolone u finalnom DataFrame-u:")
        print(df.columns)

        # Provjera duplikata na osnovu ID-a
        initial_count = len(df)

        # Koristimo kolonu 'id' (ili 'ID' ako se ponekad pojavljuje velikim slovima)
        id_column = 'id' if 'id' in df.columns else 'ID'
        df.drop_duplicates(subset=[id_column], inplace=True, keep='first')

        final_count = len(df)
        print(f"\nUklonjeno {initial_count - final_count} duplikata na osnovu kolone '{id_column}'.")
        print(f"KONAČAN BROJ JEDINSTVENIH ČLANAKA: {len(df)}")


    else:
        print("\nNijedan DataFrame nije kreiran. Provjerite da li su fajlovi ispravnog formata.")

# ==============================================================================
# FAZA 3: PRIPREMA PODATAKA ZA MODEL
# ==============================================================================
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict

# 3.1. Čišćenje i filtriranje
# Ovdje koristimo tačna imena kolona iz JSON fajla.

# Ovdje biramo koji sažetak želimo koristiti kao cilj (labelu).
# Ja sam ostavio 'claude_sumarizacija' kao preporuku.
summary_columns = ['claude_sumarizacija', 'chatgpt_sumarizacija', 'gemini_sumarizacija', 'ekstraktivna_sumarizacija', 'apstraktivna_sumarizacija']


# Ostavljamo samo potrebne kolone i preimenujemo ih u 'text' i 'summary'
# kako bi ostatak koda radio bez izmjena.
try:
    #df_prepared = df[['tekst', target_summary_column]].copy()
    #df_prepared.rename(columns={'tekst': 'text', target_summary_column: 'summary'}, inplace=True)

    # Uklanjamo redove gdje je tekst ili sažetak prazan/nedostaje (NaN)
    #df_prepared.dropna(subset=['text', 'summary'], inplace=True)
    # Uklanjamo redove gdje tekst ili sažetak imaju manje od 20 karaktera
    #df_prepared = df_prepared[df_prepared['text'].str.len() > 20]
    #df_prepared = df_prepared[df_prepared['summary'].str.len() > 20]

    #print(f"Broj članaka nakon čišćenja: {len(df_prepared)}")

    df_versions = []
    for col in summary_columns:
        if col in df.columns:
            temp_df = df[['tekst', col]].copy()
            temp_df.rename(columns={'tekst': 'text', col: 'summary'}, inplace=True)
            temp_df.dropna(subset=['text', 'summary'], inplace=True)
            df_versions.append(temp_df)

    # Spajamo svih 5 u jedan veliki DataFrame
    df_prepared = pd.concat(df_versions, ignore_index=True)

    # Filtar za minimum dužine
    df_prepared = df_prepared[df_prepared['text'].str.len() > 20]
    df_prepared = df_prepared[df_prepared['summary'].str.len() > 20]

    print(f"Ukupno uzoraka za treniranje sa svim sumarizacijama: {len(df_prepared)}")

    # 3.2. Podjela na train i test skupove (90% train, 10% test)
    train_df, test_df = train_test_split(df_prepared, test_size=0.1, random_state=42)

    print(f"Veličina trening skupa: {len(train_df)}")
    print(f"Veličina test skupa: {len(test_df)}")

    # 3.3. Kreiranje Hugging Face Dataset objekta
    train_dataset = Dataset.from_pandas(train_df)
    test_dataset = Dataset.from_pandas(test_df)

    raw_datasets = DatasetDict({
        'train': train_dataset,
        'test': test_dataset
    })

    print("\nDataset je spreman za tokenizaciju:")
    print(raw_datasets)

except KeyError:
    print("\nGREŠKA: Jedna od kolona ('tekst' ili '" + target_summary_column + "') nije pronađena u DataFrame-u.")
    print("Provjerite da li su imena kolona u kodu ista kao u ispisu iznad.")

# ==============================================================================
# FAZA 4: UČITAVANJE MODELA I TOKENIZERA (ISPRAVLJENA VERZIJA)
# ==============================================================================
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# Provjerimo da li je GPU dostupan, što je ključno za brzinu treniranja
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Koristi se uređaj: {device}")


# ISPRAVKA JE OVDJE:
# Koristimo 'google/mt5-small' umjesto pogrešnog imena.
# Ovo je zvanični, javno dostupan model od Google-a.
model_checkpoint = "google/mt5-small"

try:
    # Tokenizer pretvara tekst u brojeve (tokene)
    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

    # Model koji ćemo fino podesiti (fine-tune)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)

    print(f"Model '{model_checkpoint}' i njegov tokenizer su uspješno učitani.")

except Exception as e:
    print(f"Došlo je do greške prilikom učitavanja modela: {e}")
    print("\nProvjerite da li je ime modela ('model_checkpoint') tačno i da li imate pristup internetu.")


# ==============================================================================
# FAZA 5: TOKENIZACIJA (ISPRAVLJENA VERZIJA BEZ PREFIKSA)
# ==============================================================================
# Definišemo maksimalne dužine za ulaz (članak) i izlaz (sažetak)
max_input_length = 1024
max_target_length = 128
# UKLONILI SMO 'prefix' VARIJABLU

def preprocess_function(examples):
    # VIŠE NE DODAJEMO PREFIKS. Koristimo direktno tekst.
    #inputs = examples["text"]
    inputs = ["summarize: " + text for text in examples["text"]]

    # Tokeniziramo članke (ulaz)
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Tokeniziramo sažetke (cilj/labela)
    labels = tokenizer(text_target=examples["summary"], max_length=max_target_length, truncation=True)

    # Postavljamo tokenizirane sažetke kao 'labels'
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Mapiramo funkciju na cijeli dataset. 'batched=True' ubrzava proces.
# Uklanjamo stare kolone koje nam više ne trebaju.
tokenized_datasets = raw_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=raw_datasets["train"].column_names
)
print("Dataset je tokeniziran (bez prefiksa).")
print(tokenized_datasets)

# ==============================================================================
# FAZA 4: UČITAVANJE MODELA I TOKENIZERA (ISPRAVLJENA VERZIJA)
# ==============================================================================
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# Provjerimo da li je GPU dostupan, što je ključno za brzinu treniranja
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Koristi se uređaj: {device}")


# ISPRAVKA JE OVDJE:
# Koristimo 'google/mt5-small' umjesto pogrešnog imena.
# Ovo je zvanični, javno dostupan model od Google-a.
model_checkpoint = "google/mt5-small"

try:
    # Tokenizer pretvara tekst u brojeve (tokene)
    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

    # Model koji ćemo fino podesiti (fine-tune)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)

    print(f"Model '{model_checkpoint}' i njegov tokenizer su uspješno učitani.")

except Exception as e:
    print(f"Došlo je do greške prilikom učitavanja modela: {e}")
    print("\nProvjerite da li je ime modela ('model_checkpoint') tačno i da li imate pristup internetu.")


# ==============================================================================
# FAZA 5: TOKENIZACIJA (ISPRAVLJENA VERZIJA BEZ PREFIKSA)
# ==============================================================================
# Definišemo maksimalne dužine za ulaz (članak) i izlaz (sažetak)
max_input_length = 1024
max_target_length = 128
# UKLONILI SMO 'prefix' VARIJABLU

def preprocess_function(examples):
    # VIŠE NE DODAJEMO PREFIKS. Koristimo direktno tekst.
    #inputs = examples["text"]
    inputs = ["summarize: " + text for text in examples["text"]]

    # Tokeniziramo članke (ulaz)
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Tokeniziramo sažetke (cilj/labela)
    labels = tokenizer(text_target=examples["summary"], max_length=max_target_length, truncation=True)

    # Postavljamo tokenizirane sažetke kao 'labels'
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Mapiramo funkciju na cijeli dataset. 'batched=True' ubrzava proces.
# Uklanjamo stare kolone koje nam više ne trebaju.
tokenized_datasets = raw_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=raw_datasets["train"].column_names
)
print("Dataset je tokeniziran (bez prefiksa).")
print(tokenized_datasets)


# ==============================================================================
# FAZA 6: DEFINISANJE ARGUMENATA ZA TRENIRANJE (VERZIJA ZA DUŽI TRENING)
# ==============================================================================
from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer
import evaluate
import numpy as np

# Postavke za treniranje
batch_size = 4
model_name = "mt5-small-balkan-news-summarizer"
output_dir = f"./{model_name}"

# Objašnjenje strategije:
# Imamo ~180k primjera u trening setu. Jedna epoha (prolazak kroz sve podatke)
# sa batch size 4 bi trajala ~45,000 koraka, što je predugo za Colab.
# Zato ograničavamo trening na fiksni broj koraka ('max_steps').
#
# Biramo 2000 koraka kao dobar kompromis. To znači da će model vidjeti
# 2000 * 4 = 8000 članaka. To je dovoljno da se vide prvi smisleni rezultati,
# a trening bi trebao završiti unutar nekoliko sati.

# Argumenti za treniranje
args = Seq2SeqTrainingArguments(
    output_dir=output_dir,

    # Strategija evaluacije i spremanja:
    # Izvrši evaluaciju i sačuvaj model na svakih 500 koraka.
    eval_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,

    # Hiperparametri učenja:
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,

    # Ograničavanje resursa:
    save_total_limit=3,          # Čuva samo 3 zadnja checkpointa (npr. chkpt-1000, 1500, 2000)
    max_steps=2000,              # KLJUČNA PROMJENA: Ograničavamo trening na 2000 koraka

    # Tehnički detalji:
    predict_with_generate=True,  # Neophodno za generisanje teksta tokom evaluacije
    fp16=False,                   # Ubrzava trening na modernim GPU-ovima
    push_to_hub=False,           # Ne objavljujemo model na Hugging Face Hub
    report_to="none",            # Isključujemo logiranje na 'wandb'
)

# Data Collator spaja podatke iz dataseta u 'batch'-eve
# i dinamički ih 'pad'-uje (dopunjava) na istu dužinu.
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Učitavamo ROUGE metriku
rouge = evaluate.load("rouge")

# Funkcija za izračunavanje metrika tokom evaluacije
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # Dekodiramo predikcije (generisani tekst)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Zamijenimo -100 (koji se koristi za ignorisanje padding tokena u labelama) sa pad_token_id
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # Dekodiramo labele (referentni tekst)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Izračunamo ROUGE skorove
    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)

    # Dodajemo i prosječnu dužinu generisanih sažetaka kao zanimljivu metriku
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
    result["gen_len"] = np.mean(prediction_lens)

    # Zaokružujemo rezultate na 4 decimale radi preglednosti
    return {k: round(v, 4) for k, v in result.items()}

print("Faza 6 je konfigurisana za duži trening. Spremni za pokretanje Faze 7.")

# ==============================================================================
# FAZA 7: TRENIRANJE
# ==============================================================================
# Kreiramo Trainer objekat
trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"], # Koristimo test set za evaluaciju tokom treniranja
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

print("Počinje treniranje modela...")
trainer.train()


# ==============================================================================
# FAZA 8: EVALUACIJA
# ==============================================================================
print("\nPočinje finalna evaluacija na testnom skupu...")
evaluation_results = trainer.evaluate()
print("\nRezultati evaluacije:")
print(evaluation_results)

# ==============================================================================
# FAZA 9: UČITAVANJE SAČUVANOG MODELA I TESTIRANJE
# ==============================================================================
from transformers import pipeline
import os

# Putanja do direktorija gdje je Trainer spremao modele
output_dir = "./mt5-small-balkan-news-summarizer"

# Pronalazimo zadnji sačuvani checkpoint, jer on sadrži najbolju verziju modela
try:
    # Listamo sve foldere unutar output direktorija koji počinju sa 'checkpoint-'
    checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint-")]
    # Sortiramo ih numerički da nađemo zadnji (najveći broj)
    last_checkpoint = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))[-1]
    # Sastavljamo punu putanju do najboljeg modela
    model_path = os.path.join(output_dir, last_checkpoint)
    print(f"Pronađen i koristi se zadnji sačuvani model iz: {model_path}")

except (IndexError, FileNotFoundError):
    print(f"GREŠKA: Nije pronađen nijedan 'checkpoint' u direktoriju '{output_dir}'.")
    print("Provjerite da li je Faza 7 (treniranje) uspješno završena i kreirala foldere.")
    # Postavljamo model_path na None da ne bi došlo do daljih grešaka
    model_path = None


# Ako smo uspješno pronašli putanju, nastavljamo sa testiranjem
if model_path:
    # Učitamo sačuvani model direktno sa putanje
    summarizer = pipeline("summarization", model=model_path, device=0) # device=0 znači da koristi GPU

    # Uzmemo neki članak iz testnog seta za primjer
    # Moramo osigurati da 'test_df' postoji. Ako je sesija prekinuta, moramo ga ponovo kreirati.
    # Ako 'test_df' ne postoji, ova linija će izazvati NameError.
    for test_article_id in range(1, 10):
      try:
          test_article = test_df.iloc[test_article_id]['text']
          reference_summary = test_df.iloc[test_article_id]['summary']

          # Generišemo sažetak
          generated_summary = summarizer(test_article, max_length=150, min_length=30, do_sample=False)[0]['summary_text']

          print("\n--- TEST NA JEDNOM PRIMJERU ---")
          print(f"\nORIGINALNI ČLANAK:\n{test_article[:500]}...")
          print("-" * 30)
          print(f"REFERENTNI SAŽETAK (Claude):\n{reference_summary}")
          print("-" * 30)
          print(f"SAŽETAK NAŠEG MODELA:\n{generated_summary}\n\n\n")

      except NameError:
          print("\nGREŠKA: Varijabla 'test_df' nije definirana. Sesija se vjerojatno prekinula.")
          print("Molimo, ponovo pokrenite ćelije od Faze 2 i Faze 3 da biste ponovo kreirali 'test_df', a zatim se vratite i pokrenite ovu ćeliju ponovo.")
          print("Ne morate ponovo trenirati model (Faza 7).")

!zip -r model.zip ./mt5-small-balkan-news-summarizer/checkpoint-2000

from google.colab import files
files.download('model.zip')